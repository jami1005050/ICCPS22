{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "instructional-butter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.8/site-packages (0.10.2)\n",
      "Requirement already satisfied: osmnx in /opt/conda/lib/python3.8/site-packages (1.1.1)\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-0.7.1-cp38-cp38-manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from fastparquet) (1.2.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.8/site-packages (from fastparquet) (0.8.7)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 95.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thrift>=0.11.0\n",
      "  Downloading thrift-0.15.0.tar.gz (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 25.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.8/site-packages (from fastparquet) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.15.0)\n",
      "Requirement already satisfied: shapely>=1.6 in /opt/conda/lib/python3.8/site-packages (from geopandas) (1.8.0)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /opt/conda/lib/python3.8/site-packages (from geopandas) (3.2.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /opt/conda/lib/python3.8/site-packages (from geopandas) (1.8.20)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (20.3.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /opt/conda/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (49.6.0.post20210108)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (2020.12.5)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /opt/conda/lib/python3.8/site-packages (from osmnx) (3.3.4)\n",
      "Requirement already satisfied: requests>=2.25 in /opt/conda/lib/python3.8/site-packages (from osmnx) (2.25.1)\n",
      "Requirement already satisfied: networkx>=2.5 in /opt/conda/lib/python3.8/site-packages (from osmnx) (2.5)\n",
      "Requirement already satisfied: Rtree>=0.9 in /opt/conda/lib/python3.8/site-packages (from osmnx) (0.9.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3->osmnx) (8.1.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3->osmnx) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3->osmnx) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=3.3->osmnx) (1.3.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.8/site-packages (from networkx>=2.5->osmnx) (4.4.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.25->osmnx) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.25->osmnx) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.25->osmnx) (1.26.3)\n",
      "Building wheels for collected packages: thrift\n",
      "  Building wheel for thrift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for thrift: filename=thrift-0.15.0-cp38-cp38-linux_x86_64.whl size=413761 sha256=b817eb09c748c357e3ff90da89d9bc729aa390053b20e36b5c72a43bcb059da9\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/4c/b5/5b/10af165d7e0895afdfe25ad487422ae8ada6ea422b0dc444ab\n",
      "Successfully built thrift\n",
      "Installing collected packages: thrift, cramjam, fastparquet\n",
      "Successfully installed cramjam-2.4.0 fastparquet-0.7.1 thrift-0.15.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install geopandas osmnx fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-approval",
   "metadata": {},
   "source": [
    "# Process speed data\n",
    "\n",
    "* Merge 1 year's worth data to 1 dataframe per day of week.\n",
    "* And possibly just merge it again into a dataframe with a week's data\n",
    "* This notebook generates `overall_means.pkl` which is needed by almost all the subsequent notebooks\n",
    "* This notebook takes the longest time because of the loading and reading of data\n",
    "* Requires:\n",
    "    * `speed_data`\n",
    "* Generates:\n",
    "    * `overeall_means.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dedicated-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pharmaceutical-shirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deadly-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "protecting-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "\n",
    "from pprint import pprint\n",
    "from shapely.geometry import Point, LineString\n",
    "from scipy.stats import hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "starting-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from src import data_processing as data_proc\n",
    "from src.utils import Read_DF, Call_Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "emerging-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm directorys are in place\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), '../data')):\n",
    "    raise OSError(\"Must first download data, see README.md\")\n",
    "data_dir = os.path.join(os.getcwd(), '../data')\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, 'speed_data')):\n",
    "    os.mkdir(os.path.join(data_dir, 'speed_data'))\n",
    "speed_dir = os.path.join(data_dir, 'speed_data')\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), '../img')):\n",
    "    raise OSError(\"Must first download data, see README.md\")\n",
    "img_dir = os.path.join(os.getcwd(), '../img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "arranged-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('^display.', silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "neural-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adjusting the editor\n",
    "# pd.set_option('display.max_columns', 15)\n",
    "# pd.set_option('display.width', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-subject",
   "metadata": {},
   "source": [
    "# Putting it all all together\n",
    "* This is the final setup\n",
    "* Using arithmetic means only\n",
    "* Added sum of incidents just in case it is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-bulgaria",
   "metadata": {},
   "source": [
    "## I. Load all the mobility dataframes together into memory\n",
    "* This step requires a lot of ram even if we are using fastparquet\n",
    "* This is done with the intention of processing the entire thing as a group/vector(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "palestinian-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "df_overall = []\n",
    "\n",
    "for month in range(1, 2):\n",
    "# for month in range(1, 13):\n",
    "    fp = os.path.join(speed_dir, f'ALL_5m_DF_2019_{month}_1.gzip')\n",
    "    DF_All = Read_DF(DF_All = fp, Reading_Tag = 'DF_All', MetaData = {})\n",
    "\n",
    "    dayofweek_df = []\n",
    "    for dayofweek in range(7):\n",
    "        df = DF_All[DF_All['time_local'].dt.dayofweek == dayofweek]\n",
    "        df = (df[df['speed_mean'].notnull()][['window', 'XDSegID', 'congestion_mean', 'speed_mean', 'Total_Number_Incidents']]\n",
    "              .groupby(['window', 'XDSegID'])\n",
    "              .agg({'congestion_mean': 'mean', 'speed_mean': 'mean', 'Total_Number_Incidents': 'sum'}))\n",
    "    \n",
    "        df['dow'] = dayofweek\n",
    "        df = df.reset_index()\n",
    "        df.set_index(['dow', 'window', 'XDSegID'], inplace=True)\n",
    "        dayofweek_df.append(df)\n",
    "\n",
    "    df_1 = deepcopy(pd.concat(dayofweek_df))\n",
    "\n",
    "    fp = os.path.join(speed_dir, f'ALL_5m_DF_2019_{month}_15.gzip')\n",
    "    DF_All = Read_DF(DF_All = fp, Reading_Tag = 'DF_All', MetaData = {})\n",
    "\n",
    "    dayofweek_df = []\n",
    "    for dayofweek in range(7):\n",
    "        df = DF_All[DF_All['time_local'].dt.dayofweek == dayofweek]\n",
    "        df = (df[df['speed_mean'].notnull()][['window', 'XDSegID', 'congestion_mean', 'speed_mean', 'Total_Number_Incidents']]\n",
    "              .groupby(['window', 'XDSegID'])\n",
    "              .agg({'congestion_mean': 'mean', 'speed_mean': 'mean', 'Total_Number_Incidents': 'sum'}))\n",
    "        df['dow'] = dayofweek\n",
    "        df = df.reset_index()\n",
    "        df.set_index(['dow', 'window', 'XDSegID'], inplace=True)\n",
    "        dayofweek_df.append(df)\n",
    "\n",
    "    df_15 = deepcopy(pd.concat(dayofweek_df))\n",
    "\n",
    "    df_month_all = pd.concat([df_1.reset_index(), df_15.reset_index()])\n",
    "\n",
    "    df_month_all = (df_month_all\n",
    "                    .groupby(['dow', 'window', 'XDSegID'], as_index=False)\n",
    "                    .agg({'congestion_mean': 'mean', 'speed_mean': 'mean', 'Total_Number_Incidents': 'sum'}))\n",
    "    df_month_all.set_index(['dow', 'window', 'XDSegID'], inplace=True)\n",
    "\n",
    "    df_overall.append(df_month_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-tribute",
   "metadata": {},
   "source": [
    "## II. Groupby and aggregate one year's worth of data into a smaller dataframe\n",
    "* The idea is to group the data into `dayofweek`, then `time_window` and `XDSegID`\n",
    "* `congestion_mean` and `speed_mean` are averaged while the `Total_Number_Incidents` is summed.\n",
    "* This leaves us with a multi-index dataframe that allows us to easily filter out the needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reasonable-brighton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>congestion_mean</th>\n",
       "      <th>speed_mean</th>\n",
       "      <th>Total_Number_Incidents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow</th>\n",
       "      <th>window</th>\n",
       "      <th>XDSegID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>155791217</th>\n",
       "      <td>0.204545</td>\n",
       "      <td>17.7500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155795795</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.7500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155796708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155802488</th>\n",
       "      <td>0.014706</td>\n",
       "      <td>20.5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155826562</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>14.7500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">6</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">287</th>\n",
       "      <th>1524644585</th>\n",
       "      <td>0.000806</td>\n",
       "      <td>62.5050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524645372</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.7500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524646416</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.2300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524646417</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.1225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524646899</th>\n",
       "      <td>0.026176</td>\n",
       "      <td>36.1100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10995264 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       congestion_mean  speed_mean  Total_Number_Incidents\n",
       "dow window XDSegID                                                        \n",
       "0   0      155791217          0.204545     17.7500                       0\n",
       "           155795795          0.000000     10.7500                       0\n",
       "           155796708          0.000000     17.5000                       0\n",
       "           155802488          0.014706     20.5000                       0\n",
       "           155826562          0.016667     14.7500                       0\n",
       "...                                ...         ...                     ...\n",
       "6   287    1524644585         0.000806     62.5050                       0\n",
       "           1524645372         0.000000     26.7500                       0\n",
       "           1524646416         0.000000     58.2300                       0\n",
       "           1524646417         0.000000     58.1225                       0\n",
       "           1524646899         0.026176     36.1100                       0\n",
       "\n",
       "[10995264 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overall_all = pd.concat([df.reset_index() for df in df_overall])\n",
    "df_overall_all = (df_overall_all\n",
    "                 .groupby(['dow', 'window', 'XDSegID'], as_index=False)\n",
    "                 .agg({'congestion_mean': 'mean', 'speed_mean': 'mean', 'Total_Number_Incidents': 'sum'}))\n",
    "df_overall_all.set_index(['dow', 'window', 'XDSegID'], inplace=True)\n",
    "df_overall_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-trigger",
   "metadata": {},
   "source": [
    "### II.b: Saving the file\n",
    "* This will be used in the next notebook: `003b_Creating_Clusters_correct.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(data_dir, 'overall_means.pkl')\n",
    "df_overall_all.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-convertible",
   "metadata": {},
   "source": [
    "# All other cells below are just helpers or debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-greeting",
   "metadata": {},
   "source": [
    "# III. Accessing the multi-index dataframe `df_overall_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accessing a day of week using .loc\n",
    "df_overall_all.loc[[(0)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accessing specific day of week.\n",
    "dayofweek = 5\n",
    "# drop_level is set to true by default, setting it to false, will keep the 'dow' column\n",
    "_temp = df_overall_all.xs((dayofweek, ), level=['dow'], drop_level=False)\n",
    "display(_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accessing specific time window\n",
    "_temp = df_overall_all.xs((0, 0), level=['dow', 'window'], drop_level=True)\n",
    "display(_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accessing a segment for a day of week\n",
    "speeds = df_overall_all.xs((0, 1524646899), level=['dow', 'XDSegID'], drop_level=True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(speeds['speed_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-shakespeare",
   "metadata": {},
   "source": [
    "### III.b Accessing based on specific times (time start, time end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_slice_from_overall(overall_df, dayofweek, time_start, time_end, date='2019-01-01', segment=None, granularity=5):\n",
    "    # Always has 5 min granularity\n",
    "    time_windows = data_proc.get_number_of_time_windows(granularity)\n",
    "\n",
    "    start_time = pd.Timestamp(time_start)\n",
    "    end_time = pd.Timestamp(time_end)\n",
    "    start_window = data_proc.time_window_from_time(start_time, time_windows)\n",
    "    end_window = data_proc.time_window_from_time(end_time, time_windows)\n",
    "    \n",
    "    if segment:\n",
    "        _slice = overall_df.loc[(dayofweek, start_window):(dayofweek, end_window)].xs((segment,), level=['XDSegID']).droplevel(0)\n",
    "        \n",
    "        start_time_str = start_time.strftime(time_start)\n",
    "        datetime_arr = data_proc.generate_datetime_arr(start_time_str, len(_slice), granularity, date=date)\n",
    "        _slice['time_local'] = datetime_arr\n",
    "        _slice = _slice.reset_index()\n",
    "        _slice.set_index('time_local', inplace=True)\n",
    "        return _slice\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '06:00:00'\n",
    "end_time = '21:00:00'\n",
    "granularity = 5\n",
    "dayofweek = 0\n",
    "speeds = get_data_slice_from_overall(df_overall_all, dayofweek, start_time, end_time, segment=1524646899, granularity=granularity)\n",
    "speeds.plot(y='congestion_mean', figsize=(10, 3), marker='o', markevery=2, markersize=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_granularity = 15\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "for dayofweek in range(7):\n",
    "    speeds = get_data_slice_from_overall(df_overall_all, dayofweek, start_time, end_time, segment=1524646899, granularity=granularity)\n",
    "    \n",
    "    speeds = speeds.groupby(pd.Grouper(freq=f'{new_granularity}T')).agg({'speed_mean':'mean', 'congestion_mean':'mean', 'Total_Number_Incidents': 'sum'})\n",
    "#     speeds.plot(y='speed_mean', ax=ax)\n",
    "    speeds.plot(y='congestion_mean', marker='o', markevery=5, markersize=4, ax=ax, lw=0.5, legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-tradition",
   "metadata": {},
   "source": [
    "## Accessing and adding time aspect column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowing that the granularity of the data is 5 minutes per time window, we can factor that in to generate datetime arr\n",
    "granularity = 5\n",
    "total_count = int(24 * 60 / granularity)\n",
    "print(total_count)\n",
    "datetime_arr = data_proc.generate_datetime_arr('00:00:00', total_count, granularity, date='2019-01-01')\n",
    "\n",
    "# Get a time window for a day of week\n",
    "speeds = df_overall_all.xs((0, 1524646899), level=['dow', 'XDSegID'], drop_level=True)\n",
    "speeds['datetime'] = datetime_arr\n",
    "\n",
    "speeds = speeds.set_index('datetime')\n",
    "display(speeds)\n",
    "\n",
    "speeds.plot(y='speed_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to show resampling for hmean (but a bit finicky with edges.)\n",
    "speeds = speeds.groupby(pd.Grouper(freq='15T')).agg({'speed_mean':'mean', 'congestion_mean':'mean', 'Total_Number_Incidents': 'sum'})\n",
    "speeds.plot(y='speed_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-venezuela",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting both in 1 graph\n",
    "granularity = 5\n",
    "total_count = int(24 * 60 / granularity)\n",
    "print(total_count)\n",
    "datetime_arr = data_proc.generate_datetime_arr('00:00:00', total_count, granularity, date='2019-01-01')\n",
    "\n",
    "# Get a time window for a day of week\n",
    "speeds = df_overall_all.xs((0, 1524646899), level=['dow', 'XDSegID'], drop_level=True)\n",
    "speeds['datetime'] = datetime_arr\n",
    "\n",
    "speeds = speeds.set_index('datetime')\n",
    "display(speeds)\n",
    "\n",
    "new_granularity = 30\n",
    "ax = speeds.plot(y='speed_mean', figsize=(10, 3), alpha=0.4)\n",
    "speeds = speeds.groupby(pd.Grouper(freq=f'{new_granularity}T')).agg({'speed_mean':'mean', 'congestion_mean':'mean', 'Total_Number_Incidents': 'sum'})\n",
    "speeds.plot(y='speed_mean', ax=ax)\n",
    "\n",
    "# Creating twinx\n",
    "ax1 = ax.twinx()\n",
    "speeds.plot(y='congestion_mean', ax=ax1)\n",
    "\n",
    "# Fixing the x-axis label for better visibility\n",
    "# Maybe no need now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-alloy",
   "metadata": {},
   "source": [
    "## Putting it all together (all months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_start_time = '06:00:00'\n",
    "str_end_time = '21:00:00'\n",
    "\n",
    "for month in range(1, 13):\n",
    "\n",
    "    fp = os.path.join(data_dir, f'ALL_5m_DF_2019_{month}_1.gzip')\n",
    "    DF_1 = Read_DF(DF_All = fp, Reading_Tag = 'DF_All', MetaData = {})\n",
    "\n",
    "    fp = os.path.join(data_dir, f'ALL_5m_DF_2019_{month}_15.gzip')\n",
    "    DF_15 = Read_DF(DF_All = fp, Reading_Tag = 'DF_All', MetaData = {})\n",
    "\n",
    "    DF_All = pd.concat([DF_1, DF_15])\n",
    "    DF_All = DF_All[['time_local', 'XDSegID', 'Total_Number_Incidents']]\n",
    "    DF_All['time'] = DF_All['time_local'].tolist()\n",
    "    DF_All = DF_All.set_index('time_local')\n",
    "    DF_All = DF_All.between_time(str_start_time, str_end_time)\n",
    "\n",
    "    DF_Incidents = DF_All[DF_All['Total_Number_Incidents'] > 0]\n",
    "    segs = DF_Incidents['XDSegID'].tolist()\n",
    "    cluster_heads = []\n",
    "    for s in segs:\n",
    "        cluster_heads.append(locate_segment_cluster(s, clusters))\n",
    "    DF_Incidents['cluster_head'] = cluster_heads\n",
    "    fp = os.path.join(cluster_dir, f\"{clustering_version}_incidents_{str(month).zfill(2)}.pkl\")\n",
    "    DF_Incidents[DF_Incidents['cluster_head'] != -1].to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
