{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "studied-consultation",
   "metadata": {},
   "source": [
    "# Two tier anomally detection model \n",
    "Tier 1: Safe margin calculation  \n",
    "    Upper: SM(t) = Q_h + k*STD \n",
    "    Lower: SM(t) = Q_h - k*STD \n",
    "    \n",
    "Tier 2: Standard Limit calculation\n",
    "    T_max, T_min According to algorithm 1 from TDSC paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "following-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sacred-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collaborative-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "another-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime\n",
    "import random\n",
    "import importlib\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.dates as md\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from scipy.stats import hmean\n",
    "from matplotlib.lines import Line2D\n",
    "from tqdm.notebook import tqdm\n",
    "random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handy-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src') \n",
    "# without above two lines I can not import from src.common_functions \n",
    "#it throws module src not found exception \n",
    "from common_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "backed-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Anomaly_Detection_2021/training\n"
     ]
    }
   ],
   "source": [
    "# Confirm directories are in place\n",
    "print(os.path.join(os.getcwd()))\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'data_22_jun/cleaned')):\n",
    "    raise OSError(\"Must first download data, see README.md\")\n",
    "data_dir_24_may = os.path.join(os.getcwd(), 'data_22_jun/cleaned')\n",
    "\n",
    "results = os.path.join(os.getcwd(),'Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "second-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Anomaly_Detection_2021/training/data_22_jun/cleaned\n",
      "['10_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '09_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '07_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '08_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '02_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '06_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '01_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '11_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '04_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '03_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '12_2019_ratios_0027_gran_5_incidents_cleaned.pkl',\n",
      " '05_2019_ratios_0027_gran_5_incidents_cleaned.pkl']\n"
     ]
    }
   ],
   "source": [
    "print(data_dir_24_may)\n",
    "files = os.listdir(data_dir_24_may)\n",
    "pprint(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-separate",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "* Be sure to run this at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mental-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '06:00'\n",
    "end_time   = '20:55'\n",
    "training_months = (0, 8) # January to August\n",
    "cross_validation_months = (9, 10) # September and October\n",
    "testing_months = (11, 12) # November and December'\n",
    "months = {'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5,\n",
    "          'june': 6, 'july': 7, 'august': 8, 'september': 9, 'october': 10,\n",
    "          'november': 11, 'december': 12}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-george",
   "metadata": {},
   "source": [
    "# Entire Year Data \n",
    "First divide into training and testing to do the rest calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imperial-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "105108\n"
     ]
    }
   ],
   "source": [
    "info_ratio = []\n",
    "i = 0\n",
    "while i< len(files):\n",
    "    fp = os.path.join(data_dir_24_may, files[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        info_ratio.append( pickle.load(handle))\n",
    "    i+=1\n",
    "print(len(info_ratio))\n",
    "combined_ratio_frame = pd.concat(info_ratio)\n",
    "print(len(combined_ratio_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stone-kingdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43740\n"
     ]
    }
   ],
   "source": [
    "combined_ratio_frame = combined_ratio_frame.between_time(start_time, end_time)\n",
    "combined_ratio_frame =  combined_ratio_frame[(combined_ratio_frame.index.month >= months['january']) & (combined_ratio_frame.index.month <= months['august'])]\n",
    "print(len(combined_ratio_frame))\n",
    "training = combined_ratio_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-vault",
   "metadata": {},
   "source": [
    "# Select Clusters here\n",
    "* Right now it selects the 25 clusters with the most incidents throughout 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "undefined-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_version = '0027'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "marine-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all clusters\n",
    "# Confirm directories are in place\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'data_22_jun/cluster')):\n",
    "    raise OSError(\"Must first download data, see README.md\")\n",
    "cluster_info = os.path.join(os.getcwd(), 'data_22_jun/cluster')\n",
    "\n",
    "fp = os.path.join(cluster_info, f'{cluster_version}_clusters.pkl')\n",
    "with open(fp, 'rb') as handle:\n",
    "    clusters = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "institutional-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_GT= os.path.join(os.getcwd(), 'data_22_jun/incidents_GT')\n",
    "files_GT = os.listdir(fp_GT)\n",
    "incident_GT = []\n",
    "i = 0\n",
    "while i< len(files_GT):\n",
    "    fp = os.path.join(fp_GT, files_GT[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        incident_GT.append( pickle.load(handle))\n",
    "    i+=1\n",
    "incident_GT_Frame = pd.concat(incident_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "imported-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the number of clusters\n",
    "NUMBER_OF_CLUSTERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "documentary-seeker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XDSegID</th>\n",
       "      <th>Total_Number_Incidents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1524331139</th>\n",
       "      <td>108617524304</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524367555</th>\n",
       "      <td>85648069340</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524373538</th>\n",
       "      <td>99099043608</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524313548</th>\n",
       "      <td>72501459733</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   XDSegID  Total_Number_Incidents\n",
       "cluster_head                                      \n",
       "1524331139    108617524304                      82\n",
       "1524367555     85648069340                      71\n",
       "1524373538     99099043608                      65\n",
       "1524313548     72501459733                      49"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "_df = incident_GT_Frame.groupby('cluster_head').sum()\\\n",
    "                       .sort_values('Total_Number_Incidents', ascending=False)[1:].head(NUMBER_OF_CLUSTERS)\n",
    "display(_df)\n",
    "cluster_list = _df.index.tolist()\n",
    "print(len(cluster_list))\n",
    "#1524373007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-nickel",
   "metadata": {},
   "source": [
    "# Filename generation\n",
    "> Make sure you run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "consistent-funds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4C_07-27-2021'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_filename = f\"{len(cluster_list)}C_{datetime.datetime.now().strftime('%m-%d-%Y')}\"\n",
    "new_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "previous-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(results, f'used_clusters_list_{cluster_version}_{new_filename}.pkl')\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(cluster_list, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-arlington",
   "metadata": {},
   "source": [
    "# First Notebook: training_residual_and_Safe_margin.ipynb\n",
    "* Training on **January** to **August** data\n",
    "* Requires:\n",
    "    * None\n",
    "* Generates:\n",
    "    * `optimized_residual_train`\n",
    "    * `optimized_safe_margin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "combined-venice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized_safe_margin_4C_07-27-2021.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e7f54bdf63422fb6c676bc70ae2b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized_residual_train_4C_07-27-2021.pkl\n"
     ]
    }
   ],
   "source": [
    "info_ratio = []\n",
    "i = 0\n",
    "files = os.listdir(data_dir_24_may)\n",
    "while i < len(files):\n",
    "    fp = os.path.join(data_dir_24_may, files[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        info_ratio.append(pickle.load(handle))\n",
    "    i += 1\n",
    "combined_ratio_frame = pd.concat(info_ratio)\n",
    "\n",
    "combined_ratio_frame = combined_ratio_frame.between_time(start_time, end_time)\n",
    "combined_ratio_frame =  combined_ratio_frame[(combined_ratio_frame.index.month >= months['january']) & (combined_ratio_frame.index.month <= months['august'])]\n",
    "training = combined_ratio_frame\n",
    "\n",
    "training_cluster_list = training[cluster_list]\n",
    "training_cluster_list.columns = cluster_list\n",
    "Q_mean_list = {} # Qmean for each of the cluster \n",
    "for column in training_cluster_list:\n",
    "    Q_mean_list[column] = {}\n",
    "    mad = training_cluster_list[column].mad()\n",
    "    std = training_cluster_list[column].std()\n",
    "    median = training_cluster_list[column].median()\n",
    "    grouped = training_cluster_list[column].groupby([training_cluster_list[column].index.hour,\n",
    "                                                    training_cluster_list[column].index.minute])\n",
    "    Q_mean = {}\n",
    "    for key,group in grouped:\n",
    "        Q_mean[key] = group.mean()\n",
    "    Q_mean_list[column]['Q_mean'] = Q_mean\n",
    "    Q_mean_list[column]['mad'] = mad\n",
    "    Q_mean_list[column]['std'] = std\n",
    "    Q_mean_list[column]['median'] = median\n",
    "\n",
    "# generate safe_margin for all values of kappa\n",
    "kappa_L = [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0]\n",
    "safe_margin = {}\n",
    "for key in Q_mean_list.keys():\n",
    "    safe_margin[key] = {}\n",
    "    for k in kappa_L:\n",
    "        safe_margin[key][k] = {'upper':{},'lower':{}}\n",
    "        mad = Q_mean_list[key]['std']\n",
    "\n",
    "        Q_mean = Q_mean_list[key]['Q_mean']\n",
    "        for key1 in Q_mean.keys(): \n",
    "            safe_margin[key][k]['upper'][key1] = Q_mean[key1] + mad * k\n",
    "            safe_margin[key][k]['lower'][key1] = Q_mean[key1] - mad * k\n",
    "\n",
    "fp = os.path.join(results, f'optimized_safe_margin_{new_filename}.pkl')\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(safe_margin, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f'Saved optimized_safe_margin_{new_filename}.pkl')\n",
    "    \n",
    "residual = {}\n",
    "\n",
    "for column in tqdm(training_cluster_list.columns):\n",
    "    grouped = training_cluster_list[column].groupby([training_cluster_list[column].index.hour,\n",
    "                                                     training_cluster_list[column].index.minute])\n",
    "    sm_per_C = safe_margin[column]\n",
    "    R_per_C = {}\n",
    "    for key in sm_per_C.keys():\n",
    "        nabla_dict = calculate_nabla(grouped, sm_per_C[key])\n",
    "\n",
    "        nabla_frame = pd.DataFrame(list(nabla_dict.items()),columns = ['time','nabla'])\n",
    "        nabla_frame.set_index('time', inplace=True)\n",
    "        SF_List = [3,5,7,9]\n",
    "        RUC = {}\n",
    "        for sf in SF_List:\n",
    "            RUC[sf] = faster_calculate_residual(nabla_frame,sf)\n",
    "        R_per_C[key] = RUC\n",
    "\n",
    "    residual[column] = R_per_C\n",
    "\n",
    "fp = os.path.join(results, f'optimized_residual_train_{new_filename}.pkl')\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(residual, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f'Saved optimized_residual_train_{new_filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-transparency",
   "metadata": {},
   "source": [
    "# Second Notebook: standar_limit_QR.ipynb\n",
    "* Calculates $\\tau_{min}$ and $\\tau_{max}$\n",
    "* Uses the new algorithms in the utility.py\n",
    "* Training on **January** to **August** data\n",
    "* Requires:\n",
    "    * `optimized_residual_train`\n",
    "    * `optimized_safe_margin`\n",
    "* Generates:\n",
    "    * `optimized_standard_limit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "wireless-midwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe992aa6afc74cf1aea3ae1a26b3a6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized_standard_limit_4C_07-27-2021_huber.pkl\n"
     ]
    }
   ],
   "source": [
    "fp_residual = os.path.join(results, f'optimized_residual_train_{new_filename}.pkl')\n",
    "with open(fp_residual, 'rb') as handle:\n",
    "    residual = pickle.load(handle)\n",
    "\n",
    "fp_safe_margin = os.path.join(results, f'optimized_safe_margin_{new_filename}.pkl')\n",
    "with open(fp_safe_margin, 'rb') as handle:\n",
    "    safe_margin = pickle.load(handle)\n",
    "\n",
    "residual_filtered = {}\n",
    "for key in residual.keys():\n",
    "    if(key in cluster_list):\n",
    "        residual_filtered[key] = residual[key]\n",
    "\n",
    "safe_margin_filtered = {}\n",
    "for key in safe_margin.keys():\n",
    "    if(key in cluster_list):\n",
    "        safe_margin_filtered[key] = safe_margin[key]\n",
    "\n",
    "df = pd.DataFrame.from_dict(residual_filtered, orient=\"index\").stack().to_frame()\n",
    "df = pd.DataFrame(df[0].values.tolist(), index=df.index)\n",
    "indices = df.index.tolist()\n",
    "sf_keys = df.columns.tolist()\n",
    "\n",
    "standard_limit = []\n",
    "# beta_list = [.001,.002,.003,.004,.005,.006,.007,.008,.009,\n",
    "#              .01,.02,.03,.04,.05,.06,0.07,.08,.09,\n",
    "#              .1,.2,.3,.4,.5,.6,.7,.9]\n",
    "beta_list = [0.001, 0.005, 0.05, 0.5, 1.5]\n",
    "\n",
    "pbar = tqdm(total=(len(indices) * len(sf_keys)))\n",
    "for index in indices:\n",
    "    for sf_key in sf_keys:\n",
    "        _df = pd.DataFrame.from_dict(df.loc[index][sf_key].items())\n",
    "        _df = _df.rename(columns={0:'time', 1: 'nabla'})\n",
    "        _df.set_index('time', inplace=True)\n",
    "#         max_nabla =abs( _df['nabla'].max())\n",
    "#         print(max_nabla)\n",
    "#         beta_list = list(np.arange(0.00025, max_nabla, 0.00025))\n",
    "#         print(beta_list)\n",
    "#         weight_list = [0.1,0.2,0.3,0.4]#list(np.arange(0,1,0.1))\n",
    "#         pbar_1 = tqdm(total=(len(beta_list))\n",
    "        for beta in beta_list:\n",
    "#             for w in weight_list: #w1*w2 = 1 or w1+w2 = 1\n",
    "            w1 = 0.5\n",
    "            w2 = 2\n",
    "            T_max = calculate_tmax_huber(_df['nabla'],beta,w1,w2)#for huber \n",
    "            T_min = calculate_tmin_huber(_df['nabla'],beta,w1,w2) #for huber only\n",
    "            temp = {'cluster_id':index[0],\n",
    "                    'ka ppa':index[1],\n",
    "                    'SF':sf_key,\n",
    "                    'tau_max':T_max, 'tau_min':T_min,\n",
    "                    'beta':beta,'w1':w1,'w2':w2}\n",
    "            standard_limit.append(temp)\n",
    "#             break\n",
    "#             pbar_1.update()\n",
    "#           pbar_1.close()\n",
    "        pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f'optimized_standard_limit_{new_filename}_huber.pkl')\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(standard_limit, handle)\n",
    "    print(f'Saved optimized_standard_limit_{new_filename}_huber.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-acoustic",
   "metadata": {},
   "source": [
    "# Third Notebook: cross_validate_residual.ipynb\n",
    "* Cross validating on **September** and **October** data\n",
    "* Requires:\n",
    "    * `optimized_safe_margin`\n",
    "* Generates:\n",
    "    * `optimized_test_residual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "increased-magnitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fc83ced6f943448e88b3b8af366253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized_test_residual_4C_07-27-2021.pkl\n"
     ]
    }
   ],
   "source": [
    "fp_safe_margin = os.path.join(results, f'optimized_safe_margin_{new_filename}.pkl')\n",
    "with open(fp_safe_margin, 'rb') as handle:\n",
    "    safe_margin = pickle.load(handle)\n",
    "\n",
    "test_data_dir_24_may = os.path.join(os.getcwd(), 'data_22_jun/incident')\n",
    "test_files = os.listdir(test_data_dir_24_may)\n",
    "info_ratio_incidents = []\n",
    "i = 0\n",
    "while i< len(test_files):\n",
    "    fp = os.path.join(test_data_dir_24_may, test_files[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        info_ratio_incidents.append( pickle.load(handle))\n",
    "    i+=1\n",
    "combined_ratio_frame_incidents = pd.concat(info_ratio_incidents)\n",
    "\n",
    "combined_ratio_frame_incidents = combined_ratio_frame_incidents.between_time(start_time, end_time)\n",
    "combined_ratio_frame_incidents = combined_ratio_frame_incidents[(combined_ratio_frame_incidents.index.month >= months['september']) \n",
    "                                                              & (combined_ratio_frame_incidents.index.month <= months['october'])]\n",
    "\n",
    "testing = combined_ratio_frame_incidents\n",
    "testing_Clist = testing[cluster_list]\n",
    "testing_Clist.columns = cluster_list\n",
    "\n",
    "test_residual = {}\n",
    "\n",
    "for column in tqdm(testing_Clist.columns):\n",
    "    grouped = testing_Clist[column].groupby([testing_Clist[column].index.hour,\n",
    "                                             testing_Clist[column].index.minute])\n",
    "    sm_per_C = safe_margin[column]\n",
    "    R_per_C = {}\n",
    "    for key in sm_per_C.keys():\n",
    "        nabla_dict = calculate_nabla(grouped, sm_per_C[key])\n",
    "\n",
    "        nabla_frame = pd.DataFrame(list(nabla_dict.items()),columns = ['time','nabla'])\n",
    "        nabla_frame.set_index('time', inplace=True)\n",
    "        SF_List = [3,5,7,9]\n",
    "        RUC = {}\n",
    "        for sf in SF_List:\n",
    "            RUC[sf] = faster_calculate_residual(nabla_frame,sf)\n",
    "        R_per_C[key] = RUC\n",
    "\n",
    "    test_residual[column] = R_per_C\n",
    "\n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f'optimized_test_residual_{new_filename}.pkl')\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(test_residual, handle)\n",
    "    print(f'Saved optimized_test_residual_{new_filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-confidentiality",
   "metadata": {},
   "source": [
    "# Fourth Notebook: detection_QR.ipynb\n",
    "* Cross validating on **September** and **October** data\n",
    "* Requires:\n",
    "    * `optimized_safe_margin`\n",
    "    * `optimized_standard_limit`\n",
    "    * `optimized_test_residual`\n",
    "* Generates:\n",
    "    * `optimized_detection_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-portrait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35995345f534792aeb1bcc28abd1f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized_detection_report_4C_07-27-2021.pkl\n"
     ]
    }
   ],
   "source": [
    "fp_safe_margin = os.path.join(results, f'optimized_safe_margin_{new_filename}.pkl')\n",
    "with open(fp_safe_margin, 'rb') as handle:\n",
    "    safe_margin = pickle.load(handle)\n",
    "\n",
    "fp_standard_limit = os.path.join(results, f'optimized_standard_limit_{new_filename}_huber.pkl')\n",
    "with open(fp_standard_limit, 'rb') as handle:\n",
    "    standard_limit_5C = pickle.load(handle)\n",
    "standard_limit_5C_Frame = pd.DataFrame(standard_limit_5C)\n",
    "# print(standard_limit_5C_Frame)\n",
    "fp_test_res = os.path.join(results, f'optimized_test_residual_{new_filename}.pkl')\n",
    "with open(fp_test_res, 'rb') as handle:\n",
    "    test_residual = pickle.load(handle)\n",
    "\n",
    "info_ratio_incidents = []\n",
    "i = 0\n",
    "while i< len(test_files):\n",
    "    fp = os.path.join(test_data_dir_24_may, test_files[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        info_ratio_incidents.append( pickle.load(handle))\n",
    "    i+=1\n",
    "combined_ratio_frame_incidents = pd.concat(info_ratio_incidents)\n",
    "\n",
    "testing = combined_ratio_frame_incidents.between_time(start_time, end_time)\n",
    "testing =  testing[(testing.index.month >= months['september']) & (testing.index.month <= months['october']) ]\n",
    "testing_Clist = testing[cluster_list]\n",
    "testing_Clist.columns = cluster_list\n",
    "\n",
    "detection_report = []\n",
    "for column in tqdm(testing_Clist.columns):\n",
    "    grouped = testing_Clist[column].groupby([testing_Clist[column].index.hour,\n",
    "                                             testing_Clist[column].index.minute])\n",
    "\n",
    "    sm_per_C = safe_margin[column] # safe margin list for each cluster\n",
    "    for key in sm_per_C.keys(): # for each safe margin\n",
    "        for key1, group in grouped:\n",
    "            group = group.dropna()\n",
    "\n",
    "            groupDF = pd.DataFrame(group)\n",
    "            groupDF['g_upper'] = groupDF[column] > sm_per_C[key]['upper'][key1]\n",
    "            groupDF['l_lower'] = groupDF[column] < sm_per_C[key]['lower'][key1]\n",
    "            groupDF['or'] = groupDF['g_upper'] | groupDF['l_lower']\n",
    "\n",
    "            groupDF = groupDF[groupDF['or'] == True]\n",
    "            res_SF = test_residual[column][key]\n",
    "            for key2 in res_SF.keys():\n",
    "                std_limit = standard_limit_5C_Frame[(standard_limit_5C_Frame['cluster_id']== column) &\n",
    "                                                    (standard_limit_5C_Frame['ka ppa']== key) &\n",
    "                                                    (standard_limit_5C_Frame['SF']== key2)]\n",
    "#                 print(std_limit)\n",
    "#                 index_ar = std_limit.index\n",
    "                for tau in std_limit.itertuples(): # this loop is added to iterate over different beta values\n",
    "#                     print(tau.tau_max,' ',tau.tau_min)\n",
    "                    for index, row in groupDF.iterrows():\n",
    "                        temp = None\n",
    "                        if(res_SF[key2][index] >0):\n",
    "                            if(res_SF[key2][index]>tau.tau_max):\n",
    "                                temp = {'cluster_id':column,'kappa':key,'SF':key2,'beta':tau.beta,\n",
    "                                        'time':index,'RUC':res_SF[key2][index],'tau_max':tau.tau_max}\n",
    "                                detection_report.append(temp)\n",
    "                        else:\n",
    "                            if(res_SF[key2][index]<tau.tau_min):\n",
    "                                temp = {'cluster_id':column,'kappa':key,'SF':key2,'time':index,'beta':tau.beta,\n",
    "                                        'RUC':res_SF[key2][index],'tau_min':tau.tau_min}\n",
    "                                detection_report.append(temp)\n",
    "\n",
    "detection_report_Frame = pd.DataFrame(detection_report)\n",
    "detection_report_Frame.set_index('time',inplace = True)\n",
    "\n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f\"optimized_detection_report_{new_filename}.pkl\")\n",
    "detection_report_Frame.to_pickle(fp)\n",
    "print(f\"Saved optimized_detection_report_{new_filename}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-night",
   "metadata": {},
   "source": [
    "# Fifth Notebook: analyse_detection_Q_Huber.ipynb\n",
    "* Cross validating on **September** and **October** data\n",
    "* Requires:\n",
    "    * `optimized_detection_report`\n",
    "* Generates:\n",
    "    * `optimized_hyper_mapping`\n",
    "    * `optimized_actual_detection_Frame`: For use with graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-conspiracy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598230f570b34334afd60693785e19c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6d8c8cbe6d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mactual_detection_Frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'detection_number'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mhyper_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid call for scalar access (setting)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2165\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtake_split_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0;31m# We have to operate column-wise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0;31m# scalar value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0milocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1720\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_with_indexer_2d_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0;31m# reset the sliced object if unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iset_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_iset_item\u001b[0;34m(self, loc, value)\u001b[0m\n\u001b[1;32m   3221\u001b[0m         \u001b[0;31m#  but the behavior is the same as long as we pass broadcast=False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3222\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3223\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iset_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3225\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_iset_item\u001b[0;34m(self, loc, value)\u001b[0m\n\u001b[1;32m   3819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iset_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3822\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36miset\u001b[0;34m(self, loc, value)\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0mblk_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                 \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0munfit_mgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mset_inplace\u001b[0;34m(self, locs, values)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mcreate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malways\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fp_detection_report = os.path.join(results, f\"optimized_detection_report_{new_filename}.pkl\")\n",
    "with open(fp_detection_report, 'rb') as handle:\n",
    "    detection_report_Frame = pickle.load(handle)\n",
    "\n",
    "testing_incident_GT = incident_GT_Frame.between_time(start_time, end_time)\n",
    "testing_incident_GT = testing_incident_GT[(testing_incident_GT.index.month >= months['september']) & (testing_incident_GT.index.month <= months['october'])]\n",
    "testing_incident_GT_Clist = testing_incident_GT[testing_incident_GT['cluster_head'].isin(cluster_list)]\n",
    "\n",
    "group_detection_report_by_cluster_id = detection_report_Frame.groupby('cluster_id')\n",
    "group_gt_incident_cluster_head = testing_incident_GT_Clist.groupby('cluster_head')\n",
    "actual_detection = []\n",
    "detection_GT = []\n",
    "for key, gorup in tqdm(group_detection_report_by_cluster_id):\n",
    "    group_by_kappa_sf = gorup.groupby(['kappa','SF','beta']) # here the beta is newly added parameter \n",
    "    for (key1,key2,key3), group in group_by_kappa_sf:\n",
    "        for index,row in group.iterrows():\n",
    "            detection_type = 0\n",
    "            if key in group_gt_incident_cluster_head.groups.keys():\n",
    "                for index1,row1 in group_gt_incident_cluster_head.get_group(key).iterrows():\n",
    "                    #iterate only incidents happend for the cluster \n",
    "                    if((index.month == index1.month) and (index.day == index1.day)):\n",
    "                        #This means incident and detection are on the same day\n",
    "                        if((index.hour >= (index1.hour-2)) & (index.hour <= (index1.hour+2))):\n",
    "                            #this means successful detection of the incident\n",
    "                            detection_type = 1\n",
    "                            temp1 = {'cluster_id':key,'kappa':key1,'SF':key2,'time':index1,'beta':key3}\n",
    "                            detection_GT.append(temp1)\n",
    "                        elif((index.hour >= 6) & (index.hour <= 10) or\n",
    "                            (index.hour >= 16) & (index.hour <= 18)):\n",
    "                            #this means detected an incident\n",
    "                            detection_type = 2\n",
    "                        else:\n",
    "                            detection_type =3\n",
    "                        break\n",
    "                temp = {'cluster_id':key,'kappa':key1,'SF':key2,'time':index,'detection_type':detection_type,'beta':key3}\n",
    "                actual_detection.append(temp)\n",
    "\n",
    "actual_detection_Frame = pd.DataFrame(actual_detection)\n",
    "actual_detection_Frame.set_index('time',inplace = True)\n",
    "detection_GT_Frame = pd.DataFrame(detection_GT)\n",
    "detection_GT_Frame.set_index('time',inplace = True)\n",
    "\n",
    "actual_detection_Frame['detection_number'] = 0\n",
    "group_actual_detection_Frame = actual_detection_Frame.groupby(['cluster_id'])\n",
    "for key1, group in tqdm(group_actual_detection_Frame):\n",
    "    group_c = group.groupby(['kappa','SF','beta'])\n",
    "    for (key2, key3,key4), grp in group_c:\n",
    "        current = None\n",
    "        detection = 0\n",
    "        grp.sort_index(inplace=True)\n",
    "        for index,item in grp.iterrows():\n",
    "            if((current == None)):\n",
    "                current = index\n",
    "            else:\n",
    "                if((current.month == index.month) & (current.day == index.day)):\n",
    "                    if(current.hour == index.hour):\n",
    "                        diff = index.minute - current.minute\n",
    "                        if(diff == 5):\n",
    "                            grp.at[index,'detection_number'] = detection\n",
    "                            current = index\n",
    "                            continue\n",
    "                        else:\n",
    "                            detection = detection + 1\n",
    "                    else:\n",
    "                        H_diff = index.hour - current.hour\n",
    "                        if(H_diff == 1):\n",
    "                            if((index.minute  == 0) & (current.minute == 55)):\n",
    "                                grp.at[index,'detection_number'] = detection\n",
    "                                current = index\n",
    "                                continue\n",
    "                            else:\n",
    "                                detection = detection + 1\n",
    "                        else:\n",
    "                            detection = detection + 1\n",
    "                else: \n",
    "                    detection = detection + 1\n",
    "                grp.at[index,'detection_number'] = detection\n",
    "                current = index\n",
    "        for index,item in grp.iterrows():\n",
    "            actual_detection_Frame.at[index,'detection_number'] = item.detection_number\n",
    "\n",
    "hyper_mapping = {}\n",
    "group_actual_detection_Frame = actual_detection_Frame.groupby(['cluster_id'])\n",
    "for key1, group in tqdm(group_actual_detection_Frame):\n",
    "    group_c = group.groupby(['kappa','SF','beta'])\n",
    "    min_fa =  sys.maxsize\n",
    "    min_decision_fa =  (-1.0)*sys.maxsize\n",
    "    total_incident = len(testing_incident_GT_Clist[(testing_incident_GT_Clist['cluster_head']==key1)])\n",
    "    min_missed = sys.maxsize\n",
    "    print(\"CLUSTER: \",key1)\n",
    "    print('total incident: ',total_incident)\n",
    "    for (key2,key3,key4),grp in tqdm(group_c):\n",
    "        valid_detection = len(list(grp[grp['detection_type'] == 1]['detection_number'].unique())) + len(list(grp[grp['detection_type'] == 2]['detection_number'].unique()))\n",
    "        total_detection = len(list(grp['detection_number'].unique()))\n",
    "        false_alarm = total_detection - valid_detection\n",
    "        df3 = detection_GT_Frame[(detection_GT_Frame['cluster_id']==key1)&\n",
    "                                            (detection_GT_Frame['kappa']==key2)&\n",
    "                                            (detection_GT_Frame['SF']==key3) & \n",
    "                                            (detection_GT_Frame['beta']==beta)] #this beta check is added later\n",
    "        df3 = df3[~df3.index.duplicated(keep='first')]\n",
    "#         print('len(df3):',len(df3))\n",
    "        detection = len(df3)\n",
    "        fraction_of_detection = detection /total_incident\n",
    "#         print(\"fraction_of_detection: \",fraction_of_detection)\n",
    "        missed = abs(total_incident - detection)\n",
    "        fraction_FA  = false_alarm/ total_detection\n",
    "#         print('fraction_FA: ',fraction_FA)\n",
    "        decision_factor = fraction_of_detection - fraction_FA\n",
    "#         print('decision_factor:', decision_factor)\n",
    "        if((min_decision_fa < decision_factor)):\n",
    "            min_decision_fa = decision_factor\n",
    "            hyper_mapping[key1] = {'kappa':key2,'SF':key3,'beta':key4}\n",
    "#             print('false alarm: ',false_alarm)\n",
    "\n",
    "print()\n",
    "print(hyper_mapping)\n",
    "\n",
    "print()\n",
    "\n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f\"optimized_hyper_mapping_{new_filename}.pkl\")\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(hyper_mapping, handle)\n",
    "    print(f\"Saved optimized_hyper_mapping_{new_filename}.pkl\")\n",
    "\n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f\"optimized_actual_detection_Frame_{new_filename}.pkl\")\n",
    "actual_detection_Frame.to_pickle(fp)\n",
    "print(f\"Saved optimized_actual_detection_Frame_{new_filename}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-quarterly",
   "metadata": {},
   "source": [
    "# Sixth Notebook: test_residual_QR.ipynb\n",
    "* Cross validating on **October**, **November**, and **December** data\n",
    "* Requires:\n",
    "    * `optimized_safe_margin`\n",
    "    * `optimized_hyper_mapping`\n",
    "* Generates:\n",
    "    * `optimized_residual`: Not to be confused with `optimized_test_residual` generated in notebook three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validated_kappa_SF = hyper_mapping\n",
    "fp_safe_margin = os.path.join(results, f'optimized_safe_margin_{new_filename}.pkl')\n",
    "with open(fp_safe_margin, 'rb') as handle:\n",
    "    safe_margin = pickle.load(handle)\n",
    "\n",
    "test_data_dir_24_may = os.path.join(os.getcwd(), 'data_22_jun/incident')\n",
    "test_files = os.listdir(test_data_dir_24_may)\n",
    "info_ratio_incidents = []\n",
    "i = 0\n",
    "while i< len(test_files):\n",
    "    fp = os.path.join(test_data_dir_24_may, test_files[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        info_ratio_incidents.append( pickle.load(handle))\n",
    "    i+=1\n",
    "combined_ratio_frame_incidents = pd.concat(info_ratio_incidents)\n",
    "\n",
    "combined_ratio_frame_incidents = combined_ratio_frame_incidents.between_time(start_time, end_time)\n",
    "combined_ratio_frame_incidents = combined_ratio_frame_incidents[(combined_ratio_frame_incidents.index.month >= months['october']) \n",
    "                                                              & (combined_ratio_frame_incidents.index.month <= months['december'])]\n",
    "testing = combined_ratio_frame_incidents\n",
    "testing_Clist = testing[list(cross_validated_kappa_SF.keys())]\n",
    "testing_Clist.columns = list(cross_validated_kappa_SF.keys())\n",
    "testing_Clist.columns\n",
    "\n",
    "test_residual = {}\n",
    "for column in tqdm(testing_Clist.columns):\n",
    "    grouped = testing_Clist[column].groupby([testing_Clist[column].index.hour,\n",
    "                                                    testing_Clist[column].index.minute])\n",
    "\n",
    "    sm_per_C = safe_margin[column]\n",
    "    kappa = cross_validated_kappa_SF[column]['kappa']\n",
    "    SF = cross_validated_kappa_SF[column]['SF']\n",
    "    R_per_C = {}\n",
    "    nabla_dict = calculate_nabla(grouped,sm_per_C[kappa])\n",
    "    nabla_frame = pd.DataFrame(list(nabla_dict.items()),columns = ['time','nabla'])\n",
    "    nabla_frame.set_index('time', inplace=True)\n",
    "    _grouped = nabla_frame.groupby(nabla_frame.index.floor('D'))\n",
    "    RUC = {}\n",
    "\n",
    "    RUCsf = {}\n",
    "    for k, group in _grouped:\n",
    "        df = group.rolling(SF, min_periods=SF).sum()\n",
    "        df[0:SF] = group[0:SF]\n",
    "        _RUC = df.to_dict()['nabla']\n",
    "        RUCsf.update(_RUC)\n",
    "\n",
    "    RUC[SF] = RUCsf\n",
    "    R_per_C[kappa] = RUC\n",
    "    test_residual [column] = R_per_C\n",
    "    \n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f'optimized_residual_Test_QR_{new_filename}.pkl')\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(test_residual, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-slovenia",
   "metadata": {},
   "source": [
    "# Seventh Notebook: test_analysis_QR.ipynb\n",
    "* Cross validating on **October**, **November**, and **December** data\n",
    "* Requires:\n",
    "    * `optimized_safe_margin`\n",
    "    * `optimized_standard_limit`\n",
    "    * `optimized_residual_Test_QR`\n",
    "* Generates:\n",
    "    * `optimized_results`\n",
    "    * `optimized_actual_detection_frame`\n",
    "    * `optimized_detection_report_Frame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "fp_safe_margin = os.path.join(results, f'optimized_safe_margin_{new_filename}.pkl')\n",
    "with open(fp_safe_margin, 'rb') as handle:\n",
    "    safe_margin = pickle.load(handle)\n",
    "\n",
    "fp_standard_limit = os.path.join(results, f'optimized_standard_limit_{new_filename}_huber.pkl')\n",
    "with open(fp_standard_limit, 'rb') as handle:\n",
    "    standard_limit_5C = pickle.load(handle)\n",
    "standard_limit_5C_Frame = pd.DataFrame(standard_limit_5C)\n",
    "\n",
    "fp_test_res = os.path.join(results, f'optimized_residual_Test_QR_{new_filename}.pkl')\n",
    "with open(fp_test_res, 'rb') as handle:\n",
    "    test_residual = pickle.load(handle)\n",
    "\n",
    "test_data_dir_24_may = os.path.join(os.getcwd(), 'data_22_jun/incident')\n",
    "test_files = os.listdir(test_data_dir_24_may)\n",
    "info_ratio_incidents = []\n",
    "i = 0\n",
    "while i< len(test_files):\n",
    "    fp = os.path.join(test_data_dir_24_may, test_files[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        info_ratio_incidents.append( pickle.load(handle))\n",
    "    i+=1\n",
    "combined_ratio_frame_incidents = pd.concat(info_ratio_incidents)\n",
    "\n",
    "fp_GT= os.path.join(os.getcwd(), 'data_22_jun/incidents_GT')\n",
    "files_GT = os.listdir(fp_GT)\n",
    "incident_GT = []\n",
    "i = 0\n",
    "while i< len(files_GT):\n",
    "    fp = os.path.join(fp_GT, files_GT[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        incident_GT.append( pickle.load(handle))\n",
    "    i+=1\n",
    "incident_GT_Frame = pd.concat(incident_GT)\n",
    "cross_validated_kappa_SF = hyper_mapping\n",
    "\n",
    "testing_incident_GT = incident_GT_Frame.between_time(start_time, end_time)\n",
    "testing_incident_GT_Clist =  testing_incident_GT[testing_incident_GT['cluster_head'].isin (cluster_list)]\n",
    "testing_incident_GT_Clist =  testing_incident_GT_Clist[(testing_incident_GT_Clist.index.month >= months['october']) \n",
    "                                                     & (testing_incident_GT_Clist.index.month <= months['december'])]\n",
    "\n",
    "testing = combined_ratio_frame_incidents.between_time(start_time, end_time)\n",
    "testing =  testing[(testing.index.month>9) & (testing.index.month<=12) ]\n",
    "\n",
    "testing_Clist = testing[list(cross_validated_kappa_SF.keys())]\n",
    "testing_Clist.columns = list(cross_validated_kappa_SF.keys())\n",
    "\n",
    "detection_report = []\n",
    "for column in testing_Clist: #per cluster \n",
    "    grouped = testing_Clist[column].groupby([testing_Clist[column].index.hour,\n",
    "                                             testing_Clist[column].index.minute])\n",
    "    sm_per_C = safe_margin[column] # safe margin list for each cluster\n",
    "    kappa = cross_validated_kappa_SF[column]['kappa']\n",
    "    SF = cross_validated_kappa_SF[column]['SF']\n",
    "    beta = cross_validated_kappa_SF[column]['beta'] # cross validated beta \n",
    "    for key1, group in grouped:\n",
    "        for index, item in group.iteritems():\n",
    "            if(pd.isna(item)):continue\n",
    "            if((item > sm_per_C[kappa]['upper'][key1] ) or (item < sm_per_C[kappa]['lower'][key1] )):\n",
    "                res_SF = test_residual[column][kappa]\n",
    "                std_limit = standard_limit_5C_Frame[(standard_limit_5C_Frame['cluster_id']== column) &\n",
    "                                            (standard_limit_5C_Frame['ka ppa']== kappa) &\n",
    "                                            (standard_limit_5C_Frame['SF']== SF) &\n",
    "                                            (standard_limit_5C_Frame['beta']== beta)]\n",
    "\n",
    "                index_ar = std_limit.index\n",
    "                if(res_SF[SF][index] >0):\n",
    "                    if(res_SF[SF][index]>std_limit.at[index_ar[0],'tau_max']):\n",
    "                        temp = {'cluster_id':column,'kappa':kappa,'SF':SF,'beta':beta,\n",
    "                                'time':index,'RUC':res_SF[SF][index],'tau_max':std_limit.at[index_ar[0],'tau_max']}\n",
    "                        detection_report.append(temp)\n",
    "                else:\n",
    "                    if(res_SF[SF][index]<std_limit.at[index_ar[0],'tau_min']):\n",
    "                        temp = {'cluster_id':column,'kappa':kappa,'SF':SF,'beta':beta,\n",
    "                                'time':index,'RUC':res_SF[SF][index],'tau_min':std_limit.at[index_ar[0],'tau_min']}\n",
    "                        detection_report.append(temp)\n",
    "detection_report_Frame = pd.DataFrame(detection_report)\n",
    "detection_report_Frame.set_index('time',inplace = True)\n",
    "\n",
    "group_detection_report_by_cluster_id = detection_report_Frame.groupby('cluster_id')\n",
    "actual_detection = []\n",
    "detection_GT = []\n",
    "for key,group in group_detection_report_by_cluster_id:\n",
    "    foucsed_cluster = testing_incident_GT_Clist[testing_incident_GT_Clist['cluster_head']==key]\n",
    "    for index,row in group.iterrows():\n",
    "        detection_type = 0\n",
    "        for index1,row1 in foucsed_cluster.iterrows():\n",
    "            #iterate only incidents happend for the cluster \n",
    "            if((index.month == index1.month) and (index.day == index1.day)):\n",
    "                #This means incident and detection are on the same day\n",
    "                if((index.hour >= (index1.hour-2)) & (index.hour <= (index1.hour+2))):\n",
    "                    #this means successful detection of the incident\n",
    "                    detection_type = 1\n",
    "                    temp1 = {'cluster_id':key,'time':index1}\n",
    "                    detection_GT.append(temp1)\n",
    "                elif((index.hour >= 6) & (index.hour <= 10) or\n",
    "                    (index.hour >= 16) & (index.hour <= 18)):\n",
    "                    #this means detected an incident\n",
    "                    detection_type = 2\n",
    "                else:\n",
    "                    detection_type =3\n",
    "                break\n",
    "        temp = {'cluster_id':key,'time':index,'detection_type':detection_type}\n",
    "        actual_detection.append(temp)\n",
    "\n",
    "actual_detection_Frame = pd.DataFrame(actual_detection)\n",
    "actual_detection_Frame.set_index('time',inplace = True)\n",
    "\n",
    "actual_detection_Frame['detection_number'] = 0\n",
    "group_actual_detection_Frame = actual_detection_Frame.groupby(['cluster_id'])\n",
    "for key1, group in group_actual_detection_Frame:\n",
    "    prev = None\n",
    "    detection = 0\n",
    "    group.sort_index(inplace=True)\n",
    "    for index,item in group.iterrows():\n",
    "        if((prev == None)):\n",
    "            prev = index\n",
    "        else:\n",
    "            if((prev.month == index.month) & (prev.day == index.day)):\n",
    "                if(prev.hour == index.hour):\n",
    "                    diff = index.minute - prev.minute\n",
    "                    if(diff == 5):\n",
    "                        group.at[index,'detection_number'] = detection\n",
    "                        prev = index\n",
    "                        continue\n",
    "                    else:\n",
    "                        detection = detection + 1\n",
    "                else:\n",
    "                    H_diff = index.hour - prev.hour\n",
    "                    if(H_diff == 1):\n",
    "                        if((index.minute  == 0) & (prev.minute == 55)):\n",
    "                            group.at[index,'detection_number'] = detection\n",
    "                            prev = index\n",
    "                            continue\n",
    "                        else:\n",
    "                            detection = detection + 1\n",
    "                    else:\n",
    "                        detection = detection + 1\n",
    "            else: \n",
    "                detection = detection + 1\n",
    "            group.at[index,'detection_number'] = detection\n",
    "            prev = index\n",
    "    for index1,item in group.iterrows():\n",
    "        if(actual_detection_Frame[actual_detection_Frame['cluster_id'] == key1].at[index1,'detection_number'] == 0):\n",
    "            actual_detection_Frame.at[index1,'detection_number'] = item.detection_number\n",
    "\n",
    "report = {}\n",
    "group_by_cluster  = actual_detection_Frame.groupby('cluster_id')\n",
    "for key, group in group_by_cluster:\n",
    "    report[key] = {}\n",
    "    report[key]['cluster_id'] = key\n",
    "    print('Cluster Id: ',key)\n",
    "    total_actual_incident = len(testing_incident_GT_Clist[testing_incident_GT_Clist['cluster_head']==key])\n",
    "    print('Total Actual Incident: ',total_actual_incident)\n",
    "\n",
    "    report[key]['total_actual_incident'] = total_actual_incident\n",
    "\n",
    "    group = group[~group.index.duplicated(keep='first')]\n",
    "    total = len(list(group['detection_number'].unique()))\n",
    "    incident_frame = testing_incident_GT_Clist[testing_incident_GT_Clist['cluster_head']==key]\n",
    "    count = 0\n",
    "    print(\"incident length: \",len(incident_frame))\n",
    "    report[key]['incident_frame'] = len(incident_frame)\n",
    "\n",
    "    temp = group[group['detection_type'] == 1]\n",
    "    for index,row in incident_frame.iterrows():\n",
    "        focused_window = temp[(temp.index.month == index.month)&\n",
    "                                    (temp.index.day == index.day)&\n",
    "                                    (temp.index.hour >= (index.hour - 2))&\n",
    "                                    (temp.index.hour <= (index.hour + 2))]\n",
    "        if(len(focused_window)>0):\n",
    "            count = count + 1\n",
    "    detection = len(list(group[group['detection_type'] == 1]['detection_number'].unique()))\n",
    "    c_detection = len(list(group[group['detection_type'] == 2]['detection_number'].unique()))\n",
    "    fa_alarm  = total - detection - c_detection\n",
    "    print('total: ',total,' detection: ',detection,' c_detection: ',c_detection,' fa_alarm: ',fa_alarm,' count: ',count)\n",
    "    report[key]['results'] = {'total': total, 'detection': detection, 'c_detection': c_detection, 'fa_alarm': fa_alarm, 'count': count}\n",
    "\n",
    "# pprint(report)\n",
    "   \n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f'optimized_results_{new_filename}.pkl')\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(report, handle)\n",
    "    \n",
    "fp = os.path.join(results, f'optimized_actual_detection_frame_{new_filename}.pkl')\n",
    "actual_detection_Frame.to_pickle(fp)\n",
    "\n",
    "fp = os.path.join(results, f'optimized_detection_report_Frame_{new_filename}.pkl')\n",
    "detection_report_Frame.to_pickle(fp)\n",
    "\n",
    "elapsed_time = time.time() - time_start\n",
    "print(f\"Done in {elapsed_time} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-elements",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-litigation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
