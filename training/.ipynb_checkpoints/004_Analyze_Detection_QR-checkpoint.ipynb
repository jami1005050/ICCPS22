{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capital-affair",
   "metadata": {},
   "source": [
    "# Fifth Notebook: analyse_detection_QR.ipynb\n",
    "* Cross validating on **September** and **October** data\n",
    "* Requires:\n",
    "    * `optimized_detection_report`\n",
    "* Generates:\n",
    "    * `optimized_hyper_mapping`\n",
    "    * `optimized_actual_detection_Frame`: For use with graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "destroyed-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elect-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "equipped-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "resident-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime\n",
    "import random\n",
    "import importlib\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.dates as md\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from scipy.stats import hmean\n",
    "from matplotlib.lines import Line2D\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wired-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from src.common_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-eclipse",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "* Couldn't think of a quick solution to the cluster list since i separated notebooks. just put the length here first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "killing-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '06:00'\n",
    "end_time   = '20:55'\n",
    "training_months = (0, 8) # January to August\n",
    "cross_validation_months = (9, 10) # September and October\n",
    "testing_months = (11, 12) # November and December'\n",
    "months = {'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5,\n",
    "          'june': 6, 'july': 7, 'august': 8, 'september': 9, 'october': 10,\n",
    "          'november': 11, 'december': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coupled-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_version = '0027'\n",
    "cluster_list = [1] * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "danish-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm directories are in place\n",
    "if not os.path.exists(os.path.join(os.getcwd(), '../data')):\n",
    "    raise OSError(\"Must first download data, see README.md\")\n",
    "data_dir = os.path.join(os.getcwd(), '../data')\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, 'generated_clusters')):\n",
    "    os.mkdir(os.path.join(data_dir, 'generated_clusters'))\n",
    "cluster_dir = os.path.join(data_dir, 'generated_clusters')\n",
    "\n",
    "if not os.path.exists(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/cleaned')):\n",
    "    os.mkdir(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/cleaned'))\n",
    "cleaned_dir = os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/cleaned')\n",
    "\n",
    "if not os.path.exists(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents')):\n",
    "    os.mkdir(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents'))\n",
    "incidents_dir = os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents')\n",
    "\n",
    "if not os.path.exists(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents_GT')):\n",
    "    os.mkdir(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents_GT'))\n",
    "incidents_GT_dir = os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents_GT')\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, f'{clustering_version}_results')):\n",
    "    os.mkdir(os.path.join(data_dir, f'{clustering_version}_results'))\n",
    "results = os.path.join(data_dir, f'{clustering_version}_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-robinson",
   "metadata": {},
   "source": [
    "# Loading cluster list and regenerating filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "after-parks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0027_1C_07-09-2021'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_filename = f\"{clustering_version}_{len(cluster_list)}C_{datetime.datetime.now().strftime('%m-%d-%Y')}\"\n",
    "new_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lucky-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all clusters\n",
    "\n",
    "fp = os.path.join(cluster_dir, f'{clustering_version}_clusters.pkl')\n",
    "with open(fp, 'rb') as handle:\n",
    "    clusters = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "passive-behalf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1524373007]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join(results, f'used_clusters_list_{new_filename}.pkl')\n",
    "with open(fp, 'rb') as handle:\n",
    "    cluster_list = pickle.load(handle)\n",
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "united-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_GT = os.listdir(incidents_GT_dir)\n",
    "incident_GT = []\n",
    "i = 0\n",
    "while i< len(files_GT):\n",
    "    fp = os.path.join(incidents_GT_dir, files_GT[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        incident_GT.append( pickle.load(handle))\n",
    "    i+=1\n",
    "incident_GT_Frame = pd.concat(incident_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opposed-pound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e0813395e14ac7b1a58846f2f321e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER:  1524373007\n",
      "total incident:  11\n",
      "len(df3): 6\n",
      "fraction_of_detection:  0.5454545454545454\n",
      "fraction_FA:  0.5898617511520737\n",
      "decision_factor: -0.04440720569752832\n",
      "false alarm:  128\n",
      "len(df3): 6\n",
      "fraction_of_detection:  0.5454545454545454\n",
      "fraction_FA:  0.533678756476684\n",
      "decision_factor: 0.011775788977861468\n",
      "false alarm:  103\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.5268817204301075\n",
      "decision_factor: -0.07233626588465297\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.553763440860215\n",
      "decision_factor: -0.09921798631476048\n",
      "len(df3): 6\n",
      "fraction_of_detection:  0.5454545454545454\n",
      "fraction_FA:  0.5535714285714286\n",
      "decision_factor: -0.008116883116883189\n",
      "len(df3): 6\n",
      "fraction_of_detection:  0.5454545454545454\n",
      "fraction_FA:  0.5283018867924528\n",
      "decision_factor: 0.01715265866209259\n",
      "false alarm:  84\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.5414012738853503\n",
      "decision_factor: -0.08685581933989578\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.546583850931677\n",
      "decision_factor: -0.09203839638622252\n",
      "len(df3): 6\n",
      "fraction_of_detection:  0.5454545454545454\n",
      "fraction_FA:  0.6183206106870229\n",
      "decision_factor: -0.07286606523247752\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.49107142857142855\n",
      "decision_factor: -0.03652597402597402\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.5217391304347826\n",
      "decision_factor: -0.06719367588932806\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.5\n",
      "decision_factor: -0.04545454545454547\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.5876288659793815\n",
      "decision_factor: -0.13308341143392693\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.5833333333333334\n",
      "decision_factor: -0.12878787878787884\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.5061728395061729\n",
      "decision_factor: -0.05162738496071834\n",
      "len(df3): 4\n",
      "fraction_of_detection:  0.36363636363636365\n",
      "fraction_FA:  0.5584415584415584\n",
      "decision_factor: -0.19480519480519476\n",
      "len(df3): 5\n",
      "fraction_of_detection:  0.45454545454545453\n",
      "fraction_FA:  0.6621621621621622\n",
      "decision_factor: -0.20761670761670764\n",
      "len(df3): 4\n",
      "fraction_of_detection:  0.36363636363636365\n",
      "fraction_FA:  0.6666666666666666\n",
      "decision_factor: -0.303030303030303\n",
      "len(df3): 2\n",
      "fraction_of_detection:  0.18181818181818182\n",
      "fraction_FA:  0.6491228070175439\n",
      "decision_factor: -0.46730462519936206\n",
      "len(df3): 2\n",
      "fraction_of_detection:  0.18181818181818182\n",
      "fraction_FA:  0.6724137931034483\n",
      "decision_factor: -0.49059561128526646\n",
      "len(df3): 3\n",
      "fraction_of_detection:  0.2727272727272727\n",
      "fraction_FA:  0.6415094339622641\n",
      "decision_factor: -0.3687821612349914\n",
      "len(df3): 1\n",
      "fraction_of_detection:  0.09090909090909091\n",
      "fraction_FA:  0.7111111111111111\n",
      "decision_factor: -0.6202020202020202\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.7708333333333334\n",
      "decision_factor: -0.7708333333333334\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.7291666666666666\n",
      "decision_factor: -0.7291666666666666\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.6571428571428571\n",
      "decision_factor: -0.6571428571428571\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.6785714285714286\n",
      "decision_factor: -0.6785714285714286\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.71875\n",
      "decision_factor: -0.71875\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.696969696969697\n",
      "decision_factor: -0.696969696969697\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.7307692307692307\n",
      "decision_factor: -0.7307692307692307\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.7272727272727273\n",
      "decision_factor: -0.7272727272727273\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.7727272727272727\n",
      "decision_factor: -0.7727272727272727\n",
      "len(df3): 0\n",
      "fraction_of_detection:  0.0\n",
      "fraction_FA:  0.782608695652174\n",
      "decision_factor: -0.782608695652174\n",
      "\n",
      "{1524373007: {'kappa': 0.5, 'SF': 5}}\n",
      "\n",
      "Saved optimized_hyper_mapping_0027_1C_07-09-2021.pkl\n",
      "Saved optimized_actual_detection_Frame_0027_1C_07-09-2021.pkl\n"
     ]
    }
   ],
   "source": [
    "fp_detection_report = os.path.join(results, f\"optimized_detection_report_{new_filename}.pkl\")\n",
    "with open(fp_detection_report, 'rb') as handle:\n",
    "    detection_report_Frame = pickle.load(handle)\n",
    "\n",
    "testing_incident_GT = incident_GT_Frame.between_time(start_time, end_time)\n",
    "testing_incident_GT = testing_incident_GT[(testing_incident_GT.index.month >= months['september']) & (testing_incident_GT.index.month <= months['october'])]\n",
    "testing_incident_GT_Clist = testing_incident_GT[testing_incident_GT['cluster_head'].isin(cluster_list)]\n",
    "\n",
    "group_detection_report_by_cluster_id = detection_report_Frame.groupby('cluster_id')\n",
    "group_gt_incident_cluster_head = testing_incident_GT_Clist.groupby('cluster_head')\n",
    "actual_detection = []\n",
    "detection_GT = []\n",
    "for key, gorup in tqdm(group_detection_report_by_cluster_id):\n",
    "    group_by_kappa_sf = gorup.groupby(['kappa','SF'])\n",
    "    for (key1,key2), group in group_by_kappa_sf:\n",
    "        for index,row in group.iterrows():\n",
    "            detection_type = 0\n",
    "            if key in group_gt_incident_cluster_head.groups.keys():\n",
    "                for index1,row1 in group_gt_incident_cluster_head.get_group(key).iterrows():\n",
    "                    #iterate only incidents happend for the cluster \n",
    "                    if((index.month == index1.month) and (index.day == index1.day)):\n",
    "                        #This means incident and detection are on the same day\n",
    "                        if((index.hour >= (index1.hour-2)) & (index.hour <= (index1.hour+2))):\n",
    "                            #this means successful detection of the incident\n",
    "                            detection_type = 1\n",
    "                            temp1 = {'cluster_id':key,'kappa':key1,'SF':key2,'time':index1}\n",
    "                            detection_GT.append(temp1)\n",
    "                        elif((index.hour >= 6) & (index.hour <= 10) or\n",
    "                            (index.hour >= 16) & (index.hour <= 18)):\n",
    "                            #this means detected an incident\n",
    "                            detection_type = 2\n",
    "                        else:\n",
    "                            detection_type =3\n",
    "                        break\n",
    "                temp = {'cluster_id':key,'kappa':key1,'SF':key2,'time':index,'detection_type':detection_type}\n",
    "                actual_detection.append(temp)\n",
    "\n",
    "actual_detection_Frame = pd.DataFrame(actual_detection)\n",
    "actual_detection_Frame.set_index('time',inplace = True)\n",
    "detection_GT_Frame = pd.DataFrame(detection_GT)\n",
    "detection_GT_Frame.set_index('time',inplace = True)\n",
    "\n",
    "actual_detection_Frame['detection_number'] = 0\n",
    "group_actual_detection_Frame = actual_detection_Frame.groupby(['cluster_id'])\n",
    "for key1, group in group_actual_detection_Frame:\n",
    "    group_c = group.groupby(['kappa','SF'])\n",
    "    for (key2, key3), grp in group_c:\n",
    "        current = None\n",
    "        detection = 0\n",
    "        grp.sort_index(inplace=True)\n",
    "        for index,item in grp.iterrows():\n",
    "            if((current == None)):\n",
    "                current = index\n",
    "            else:\n",
    "                if((current.month == index.month) & (current.day == index.day)):\n",
    "                    if(current.hour == index.hour):\n",
    "                        diff = index.minute - current.minute\n",
    "                        if(diff == 5):\n",
    "                            grp.at[index,'detection_number'] = detection\n",
    "                            current = index\n",
    "                            continue\n",
    "                        else:\n",
    "                            detection = detection + 1\n",
    "                    else:\n",
    "                        H_diff = index.hour - current.hour\n",
    "                        if(H_diff == 1):\n",
    "                            if((index.minute  == 0) & (current.minute == 55)):\n",
    "                                grp.at[index,'detection_number'] = detection\n",
    "                                current = index\n",
    "                                continue\n",
    "                            else:\n",
    "                                detection = detection + 1\n",
    "                        else:\n",
    "                            detection = detection + 1\n",
    "                else: \n",
    "                    detection = detection + 1\n",
    "                grp.at[index,'detection_number'] = detection\n",
    "                current = index\n",
    "        for index,item in grp.iterrows():\n",
    "            actual_detection_Frame.at[index,'detection_number'] = item.detection_number\n",
    "\n",
    "hyper_mapping = {}\n",
    "group_actual_detection_Frame = actual_detection_Frame.groupby(['cluster_id'])\n",
    "for key1, group in group_actual_detection_Frame:\n",
    "    group_c = group.groupby(['kappa','SF'])\n",
    "    min_fa =  sys.maxsize\n",
    "    min_decision_fa =  (-1.0)*sys.maxsize\n",
    "    total_incident = len(testing_incident_GT_Clist[(testing_incident_GT_Clist['cluster_head']==key1)])\n",
    "    min_missed = sys.maxsize\n",
    "    print(\"CLUSTER: \",key1)\n",
    "    print('total incident: ',total_incident)\n",
    "    for (key2,key3),grp in group_c:\n",
    "        valid_detection = len(list(grp[grp['detection_type'] == 1]['detection_number'].unique())) + len(list(grp[grp['detection_type'] == 2]['detection_number'].unique()))\n",
    "        total_detection = len(list(grp['detection_number'].unique()))\n",
    "        false_alarm = total_detection - valid_detection\n",
    "        df3 = detection_GT_Frame[(detection_GT_Frame['cluster_id']==key1)&\n",
    "                                            (detection_GT_Frame['kappa']==key2)&\n",
    "                                            (detection_GT_Frame['SF']==key3)]\n",
    "        df3 = df3[~df3.index.duplicated(keep='first')]\n",
    "        print('len(df3):',len(df3))\n",
    "        detection = len(df3)\n",
    "        fraction_of_detection = detection /total_incident\n",
    "        print(\"fraction_of_detection: \",fraction_of_detection)\n",
    "        missed = abs(total_incident - detection)\n",
    "        fraction_FA  = false_alarm/ total_detection\n",
    "        print('fraction_FA: ',fraction_FA)\n",
    "        decision_factor = fraction_of_detection - fraction_FA\n",
    "        print('decision_factor:', decision_factor)\n",
    "        if((min_decision_fa < decision_factor)):\n",
    "            min_decision_fa = decision_factor\n",
    "            hyper_mapping[key1] = {'kappa':key2,'SF':key3}\n",
    "            print('false alarm: ',false_alarm)\n",
    "\n",
    "print()\n",
    "print(hyper_mapping)\n",
    "\n",
    "print()\n",
    "\n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f\"optimized_hyper_mapping_{new_filename}.pkl\")\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(hyper_mapping, handle)\n",
    "    print(f\"Saved optimized_hyper_mapping_{new_filename}.pkl\")\n",
    "\n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f\"optimized_actual_detection_Frame_{new_filename}.pkl\")\n",
    "actual_detection_Frame.to_pickle(fp)\n",
    "print(f\"Saved optimized_actual_detection_Frame_{new_filename}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-salad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
