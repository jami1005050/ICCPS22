{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metropolitan-interpretation",
   "metadata": {},
   "source": [
    "# Seventh Notebook: test_analysis_QR.ipynb\n",
    "* Cross validating on **October**, **November**, and **December** data\n",
    "* Requires:\n",
    "    * `optimized_safe_margin`\n",
    "    * `optimized_standard_limit`\n",
    "    * `optimized_residual_Test_QR`\n",
    "* Generates:\n",
    "    * `optimized_results`\n",
    "    * `optimized_actual_detection_frame`\n",
    "    * `optimized_detection_report_Frame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "employed-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "growing-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "seasonal-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ordered-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime\n",
    "import random\n",
    "import importlib\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.dates as md\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from scipy.stats import hmean\n",
    "from matplotlib.lines import Line2D\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dangerous-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from src.common_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-drove",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "* Couldn't think of a quick solution to the cluster list since i separated notebooks. just put the length here first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neither-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '06:00'\n",
    "end_time   = '20:55'\n",
    "training_months = (0, 8) # January to August\n",
    "cross_validation_months = (9, 10) # September and October\n",
    "testing_months = (11, 12) # November and December'\n",
    "months = {'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5,\n",
    "          'june': 6, 'july': 7, 'august': 8, 'september': 9, 'october': 10,\n",
    "          'november': 11, 'december': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chief-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_version = '0027'\n",
    "cluster_list = [1] * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blond-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm directories are in place\n",
    "if not os.path.exists(os.path.join(os.getcwd(), '../data')):\n",
    "    raise OSError(\"Must first download data, see README.md\")\n",
    "data_dir = os.path.join(os.getcwd(), '../data')\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, 'generated_clusters')):\n",
    "    os.mkdir(os.path.join(data_dir, 'generated_clusters'))\n",
    "cluster_dir = os.path.join(data_dir, 'generated_clusters')\n",
    "\n",
    "if not os.path.exists(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/cleaned')):\n",
    "    os.mkdir(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/cleaned'))\n",
    "cleaned_dir = os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/cleaned')\n",
    "\n",
    "if not os.path.exists(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents')):\n",
    "    os.mkdir(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents'))\n",
    "incidents_dir = os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents')\n",
    "\n",
    "if not os.path.exists(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents_GT')):\n",
    "    os.mkdir(os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents_GT'))\n",
    "incidents_GT_dir = os.path.join(cluster_dir, f'{clustering_version}_incident_ratios/incidents_GT')\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, f'{clustering_version}_results')):\n",
    "    os.mkdir(os.path.join(data_dir, f'{clustering_version}_results'))\n",
    "results = os.path.join(data_dir, f'{clustering_version}_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-venezuela",
   "metadata": {},
   "source": [
    "# Loading cluster list and regenerating filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fewer-ratio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0027_25C_07-14-2021'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_datetime = '07-14-2021'\n",
    "# file_datetime = datetime.datetime.now().strftime('%m-%d-%Y')\n",
    "\n",
    "new_filename = f\"{clustering_version}_{len(cluster_list)}C_{file_datetime}\"\n",
    "new_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pleasant-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_filename = f\"{clustering_version}_{len(cluster_list)}C_{datetime.datetime.now().strftime('%m-%d-%Y')}\"\n",
    "# new_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continental-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all clusters\n",
    "\n",
    "fp = os.path.join(cluster_dir, f'{clustering_version}_clusters.pkl')\n",
    "with open(fp, 'rb') as handle:\n",
    "    clusters = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "funky-wonder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1524373007,\n",
       " 1524331139,\n",
       " 1524367555,\n",
       " 1524373538,\n",
       " 1524313548,\n",
       " 449629894,\n",
       " 156110240,\n",
       " 1524356290,\n",
       " 1524276985,\n",
       " 449636438,\n",
       " 429350149,\n",
       " 1524355946,\n",
       " 1524343901,\n",
       " 160092856,\n",
       " 441552685,\n",
       " 449614988,\n",
       " 449631121,\n",
       " 1524397645,\n",
       " 1524563195,\n",
       " 1524340452,\n",
       " 449617816,\n",
       " 449614858,\n",
       " 449621051,\n",
       " 449629707,\n",
       " 441420512]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join(results, f'used_clusters_list_{new_filename}.pkl')\n",
    "with open(fp, 'rb') as handle:\n",
    "    cluster_list = pickle.load(handle)\n",
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "theoretical-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_GT = os.listdir(incidents_GT_dir)\n",
    "incident_GT = []\n",
    "i = 0\n",
    "while i< len(files_GT):\n",
    "    fp = os.path.join(incidents_GT_dir, files_GT[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        incident_GT.append( pickle.load(handle))\n",
    "    i+=1\n",
    "incident_GT_Frame = pd.concat(incident_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "colored-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(incidents_dir)\n",
    "info_ratio_incidents = []\n",
    "i = 0\n",
    "while i< len(test_files):\n",
    "    fp = os.path.join(incidents_dir, test_files[i])\n",
    "    with open(fp, 'rb') as handle:\n",
    "        info_ratio_incidents.append( pickle.load(handle))\n",
    "    i+=1\n",
    "combined_ratio_frame_incidents = pd.concat(info_ratio_incidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brave-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_safe_margin = os.path.join(results, f'optimized_safe_margin_{new_filename}.pkl')\n",
    "with open(fp_safe_margin, 'rb') as handle:\n",
    "    safe_margin = pickle.load(handle)\n",
    "\n",
    "fp_standard_limit = os.path.join(results, f'optimized_standard_limit_{new_filename}.pkl')\n",
    "with open(fp_standard_limit, 'rb') as handle:\n",
    "    standard_limit_5C = pickle.load(handle)\n",
    "standard_limit_5C_Frame = pd.DataFrame(standard_limit_5C)\n",
    "\n",
    "fp_test_res = os.path.join(results, f'optimized_residual_Test_QR_{new_filename}_test.pkl')\n",
    "with open(fp_test_res, 'rb') as handle:\n",
    "    test_residual = pickle.load(handle)\n",
    "    \n",
    "fp_safe_margin = os.path.join(results, f'optimized_hyper_mapping_{new_filename}.pkl')\n",
    "with open(fp_safe_margin, 'rb') as handle:\n",
    "    hyper_mapping = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sunset-junction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Id:  156110240\n",
      "Total Actual Incident:  12\n",
      "incident length:  12\n",
      "total:  512  detection:  29  c_detection:  27  fa_alarm:  456  count:  11\n",
      "Done in 10.507197141647339 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "cross_validated_kappa_SF = hyper_mapping\n",
    "\n",
    "testing_incident_GT = incident_GT_Frame.between_time(start_time, end_time)\n",
    "testing_incident_GT_Clist =  testing_incident_GT[testing_incident_GT['cluster_head'].isin (cluster_list)]\n",
    "testing_incident_GT_Clist =  testing_incident_GT_Clist[(testing_incident_GT_Clist.index.month >= months['october']) \n",
    "                                                     & (testing_incident_GT_Clist.index.month <= months['december'])]\n",
    "\n",
    "testing = combined_ratio_frame_incidents.between_time(start_time, end_time)\n",
    "testing =  testing[(testing.index.month>9) & (testing.index.month<=12) ]\n",
    "\n",
    "testing_Clist = testing[list(cross_validated_kappa_SF.keys())]\n",
    "testing_Clist.columns = list(cross_validated_kappa_SF.keys())\n",
    "\n",
    "detection_report = []\n",
    "for column in testing_Clist: #per cluster \n",
    "    grouped = testing_Clist[column].groupby([testing_Clist[column].index.hour,\n",
    "                                             testing_Clist[column].index.minute])\n",
    "    sm_per_C = safe_margin[column] # safe margin list for each cluster\n",
    "    kappa = cross_validated_kappa_SF[column]['kappa']\n",
    "    SF = cross_validated_kappa_SF[column]['SF']\n",
    "    for key1, group in grouped:\n",
    "        for index, item in group.iteritems():\n",
    "            if(pd.isna(item)):continue\n",
    "            if((item > sm_per_C[kappa]['upper'][key1] ) or (item < sm_per_C[kappa]['lower'][key1] )):\n",
    "                res_SF = test_residual[column][kappa]\n",
    "                std_limit = standard_limit_5C_Frame[(standard_limit_5C_Frame['cluster_id']== column) &\n",
    "                                            (standard_limit_5C_Frame['ka ppa']== kappa) &\n",
    "                                            (standard_limit_5C_Frame['SF']== SF)]\n",
    "\n",
    "                index_ar = std_limit.index\n",
    "                if(res_SF[SF][index] >0):\n",
    "                    if(res_SF[SF][index]>std_limit.at[index_ar[0],'tau_max']):\n",
    "                        temp = {'cluster_id':column,'kappa':kappa,'SF':SF,\n",
    "                                'time':index,'RUC':res_SF[SF][index],'tau_max':std_limit.at[index_ar[0],'tau_max']}\n",
    "                        detection_report.append(temp)\n",
    "                else:\n",
    "                    if(res_SF[SF][index]<std_limit.at[index_ar[0],'tau_min']):\n",
    "                        temp = {'cluster_id':column,'kappa':kappa,'SF':SF,\n",
    "                                'time':index,'RUC':res_SF[SF][index],'tau_min':std_limit.at[index_ar[0],'tau_min']}\n",
    "                        detection_report.append(temp)\n",
    "detection_report_Frame = pd.DataFrame(detection_report)\n",
    "detection_report_Frame.set_index('time',inplace = True)\n",
    "\n",
    "group_detection_report_by_cluster_id = detection_report_Frame.groupby('cluster_id')\n",
    "actual_detection = []\n",
    "detection_GT = []\n",
    "for key,group in group_detection_report_by_cluster_id:\n",
    "    foucsed_cluster = testing_incident_GT_Clist[testing_incident_GT_Clist['cluster_head']==key]\n",
    "    for index,row in group.iterrows():\n",
    "        detection_type = 0\n",
    "        for index1,row1 in foucsed_cluster.iterrows():\n",
    "            #iterate only incidents happend for the cluster \n",
    "            if((index.month == index1.month) and (index.day == index1.day)):\n",
    "                #This means incident and detection are on the same day\n",
    "                if((index.hour >= (index1.hour-2)) & (index.hour <= (index1.hour+2))):\n",
    "                    #this means successful detection of the incident\n",
    "                    detection_type = 1\n",
    "                    temp1 = {'cluster_id':key,'time':index1}\n",
    "                    detection_GT.append(temp1)\n",
    "                elif((index.hour >= 6) & (index.hour <= 10) or\n",
    "                    (index.hour >= 16) & (index.hour <= 18)):\n",
    "                    #this means detected an incident\n",
    "                    detection_type = 2\n",
    "                else:\n",
    "                    detection_type =3\n",
    "                break\n",
    "        temp = {'cluster_id':key,'time':index,'detection_type':detection_type}\n",
    "        actual_detection.append(temp)\n",
    "\n",
    "actual_detection_Frame = pd.DataFrame(actual_detection)\n",
    "actual_detection_Frame.set_index('time',inplace = True)\n",
    "\n",
    "actual_detection_Frame['detection_number'] = 0\n",
    "group_actual_detection_Frame = actual_detection_Frame.groupby(['cluster_id'])\n",
    "for key1, group in group_actual_detection_Frame:\n",
    "    prev = None\n",
    "    detection = 0\n",
    "    group.sort_index(inplace=True)\n",
    "    for index,item in group.iterrows():\n",
    "        if((prev == None)):\n",
    "            prev = index\n",
    "        else:\n",
    "            if((prev.month == index.month) & (prev.day == index.day)):\n",
    "                if(prev.hour == index.hour):\n",
    "                    diff = index.minute - prev.minute\n",
    "                    if(diff == 5):\n",
    "                        group.at[index,'detection_number'] = detection\n",
    "                        prev = index\n",
    "                        continue\n",
    "                    else:\n",
    "                        detection = detection + 1\n",
    "                else:\n",
    "                    H_diff = index.hour - prev.hour\n",
    "                    if(H_diff == 1):\n",
    "                        if((index.minute  == 0) & (prev.minute == 55)):\n",
    "                            group.at[index,'detection_number'] = detection\n",
    "                            prev = index\n",
    "                            continue\n",
    "                        else:\n",
    "                            detection = detection + 1\n",
    "                    else:\n",
    "                        detection = detection + 1\n",
    "            else: \n",
    "                detection = detection + 1\n",
    "            group.at[index,'detection_number'] = detection\n",
    "            prev = index\n",
    "    for index1,item in group.iterrows():\n",
    "        if(actual_detection_Frame[actual_detection_Frame['cluster_id'] == key1].at[index1,'detection_number'] == 0):\n",
    "            actual_detection_Frame.at[index1,'detection_number'] = item.detection_number\n",
    "\n",
    "report = {}\n",
    "group_by_cluster  = actual_detection_Frame.groupby('cluster_id')\n",
    "for key, group in group_by_cluster:\n",
    "    report[key] = {}\n",
    "    report[key]['cluster_id'] = key\n",
    "    print('Cluster Id: ',key)\n",
    "    total_actual_incident = len(testing_incident_GT_Clist[testing_incident_GT_Clist['cluster_head']==key])\n",
    "    print('Total Actual Incident: ',total_actual_incident)\n",
    "\n",
    "    report[key]['total_actual_incident'] = total_actual_incident\n",
    "\n",
    "    group = group[~group.index.duplicated(keep='first')]\n",
    "    total = len(list(group['detection_number'].unique()))\n",
    "    incident_frame = testing_incident_GT_Clist[testing_incident_GT_Clist['cluster_head']==key]\n",
    "    count = 0\n",
    "    print(\"incident length: \",len(incident_frame))\n",
    "    report[key]['incident_frame'] = len(incident_frame)\n",
    "\n",
    "    temp = group[group['detection_type'] == 1]\n",
    "    for index,row in incident_frame.iterrows():\n",
    "        focused_window = temp[(temp.index.month == index.month)&\n",
    "                                    (temp.index.day == index.day)&\n",
    "                                    (temp.index.hour >= (index.hour - 2))&\n",
    "                                    (temp.index.hour <= (index.hour + 2))]\n",
    "        if(len(focused_window)>0):\n",
    "            count = count + 1\n",
    "    detection = len(list(group[group['detection_type'] == 1]['detection_number'].unique()))\n",
    "    c_detection = len(list(group[group['detection_type'] == 2]['detection_number'].unique()))\n",
    "    fa_alarm  = total - detection - c_detection\n",
    "    print('total: ',total,' detection: ',detection,' c_detection: ',c_detection,' fa_alarm: ',fa_alarm,' count: ',count)\n",
    "    report[key]['results'] = {'total': total, 'detection': detection, 'c_detection': c_detection, 'fa_alarm': fa_alarm, 'count': count}\n",
    "\n",
    "# pprint(report)\n",
    "   \n",
    "# Saving and backing up\n",
    "fp = os.path.join(results, f'optimized_results_{new_filename}.pkl')\n",
    "with open(fp, 'wb') as handle:\n",
    "    pickle.dump(report, handle)\n",
    "    \n",
    "fp = os.path.join(results, f'optimized_actual_detection_frame_{new_filename}.pkl')\n",
    "actual_detection_Frame.to_pickle(fp)\n",
    "\n",
    "fp = os.path.join(results, f'optimized_detection_report_Frame_{new_filename}.pkl')\n",
    "detection_report_Frame.to_pickle(fp)\n",
    "\n",
    "elapsed_time = time.time() - time_start\n",
    "print(f\"Done in {elapsed_time} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-conversion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
